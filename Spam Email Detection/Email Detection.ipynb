{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1)** Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "documents = []\n",
    "labels = []\n",
    "with open('emails.csv') as f:\n",
    "    f.readline()\n",
    "    for row in f:\n",
    "        line = ''.join(row.split(',')[0:-1])[8:]\n",
    "        label= int(row.split(',')[-1].strip())\n",
    "        sentence = re.sub(r\"[^a-zA-Z]\", \" \", line.lower())\n",
    "        documents.append(sentence)\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 2)** Spliting data into random train and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_docs, test_docs, train_labels, test_labels = train_test_split(documents, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 3)** Processing training documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict  \n",
    "\n",
    "total_spam_words = 0\n",
    "total_not_spam_words = 0\n",
    "P_spam = 0\n",
    "P_not_spam = 0\n",
    "P_word_given_spam = {}  \n",
    "P_word_given_not_spam = {}\n",
    "\n",
    "vocabulary = set()  \n",
    "for doc in train_docs: \n",
    "    for word in doc.split():\n",
    "        if word.casefold() not in stop_words:\n",
    "            vocabulary.add(word)  \n",
    "\n",
    "def train_naive_bayes(documents, labels):  \n",
    "    freq = defaultdict(lambda: defaultdict(int))    \n",
    "    global total_not_spam_words\n",
    "    global total_spam_words\n",
    "    global P_spam\n",
    "    global P_not_spam\n",
    "    num_of_spams = 0  \n",
    "    num_of_not_spams = 0  \n",
    "    \n",
    "    for doc, label in zip(documents, labels):  \n",
    "        words = doc.split()  \n",
    "        if label == 1:  \n",
    "            num_of_spams += 1  \n",
    "            for word in words:  \n",
    "                if word.casefold() not in stop_words:\n",
    "                    total_spam_words += 1\n",
    "                    freq['spam'][word] += 1  \n",
    "        else:  \n",
    "            num_of_not_spams += 1  \n",
    "            for word in words:  \n",
    "                if word.casefold() not in stop_words:\n",
    "                    total_not_spam_words += 1  \n",
    "                    freq['not_spam'][word] += 1\n",
    "                \n",
    "    total_documents = num_of_spams + num_of_not_spams  \n",
    "    P_spam = num_of_spams / total_documents  \n",
    "    P_not_spam = num_of_not_spams / total_documents  \n",
    "    \n",
    "    for word in freq['spam']:  # applying Laplace smoothing\n",
    "        P_word_given_spam[word] = (freq['spam'][word] + 1) / (total_spam_words + len(vocabulary)) \n",
    "    for word in freq['not_spam']:  \n",
    "        P_word_given_not_spam[word] = (freq['not_spam'][word] + 1) / (total_not_spam_words + len(vocabulary)) \n",
    "\n",
    "\n",
    "train_naive_bayes(train_docs, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 4)** Classification and predicition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy through multiplication: 0.912739965095986\n",
      "Accuracy through summation(logarithm): 0.9921465968586387\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_by_multiplication(newEmail):  \n",
    "    words = newEmail.split()  \n",
    "    predict_spam_label = P_spam  \n",
    "    predict_not_spam_label = P_not_spam  \n",
    "    \n",
    "    for word in words:\n",
    "        if word.casefold() not in stop_words:\n",
    "            predict_spam_label *= P_word_given_spam.get(word, 1 / (total_spam_words + len(vocabulary)))\n",
    "            predict_not_spam_label *= P_word_given_not_spam.get(word, 1 / (total_not_spam_words + len(vocabulary)))  \n",
    "    \n",
    "    return 1 if predict_spam_label > predict_not_spam_label else 0  \n",
    "\n",
    "def predict_by_summation(newEmail):  \n",
    "    words = newEmail.split()  \n",
    "    log_predict_spam_label = np.log(P_spam)\n",
    "    log_predict_not_spam_label = np.log(P_not_spam)\n",
    "    \n",
    "    for word in words:\n",
    "        if word.casefold() not in stop_words:\n",
    "            log_predict_spam_label += np.log(P_word_given_spam.get(word, 1 / (total_spam_words + len(vocabulary))))\n",
    "            log_predict_not_spam_label += np.log(P_word_given_not_spam.get(word, 1 / (total_not_spam_words + len(vocabulary))))\n",
    "    \n",
    "    return 1 if log_predict_spam_label > log_predict_not_spam_label else 0\n",
    "\n",
    "\n",
    "correct_predictions_by_multiplication = 0\n",
    "correct_predictions_by_summation = 0\n",
    "for email, label in zip(test_docs, test_labels):\n",
    "    predicted_label = predict_by_multiplication(email)\n",
    "    if predicted_label == label:\n",
    "        correct_predictions_by_multiplication += 1\n",
    "\n",
    "    predicted_label = predict_by_summation(email)\n",
    "    if predicted_label == label:\n",
    "        correct_predictions_by_summation += 1\n",
    "\n",
    "print(\"Accuracy through multiplication:\", correct_predictions_by_multiplication / len(test_labels))\n",
    "print(\"Accuracy through summation(logarithm):\", correct_predictions_by_summation / len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What happens if there exists a word in the email that is not previousely processed in the BoW matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming a probability of zero for a word that is not present in the training documents can lead to several disadvantages. By way of example, If a word hasn't been seen in the training data for a particular class, assuming its probability to be zero means that any document containing that word will be classified with a probability of zero for that class. This can lead to incorrect classifications, especially when the database is limited or the vocabulary is large. Moreover, The absence of a word in the training set might be interpreted as evidence that the word is entirely unrelated to the class. This can lead to overly confident but incorrect predictions, as the model effectively ignores potentially relevant information.\n",
    "\n",
    "Not considering the probabilities of unseen words, on the other hand, makes the model less flexible and reduces its ability to generalize to new, unseen examples. This can result in poor performance on real-world data, which inevitably contains words not present in the training data. What is more, The model may misclassify documents based solely on the absence of certain words, leading to significant errors. This is particularly concerning in applications like spam detection or sentiment analysis, where specific keywords are critical to classification.\n",
    "\n",
    "in the [train_naive_bayes](#step3) function the Laplace Smoothing application is pointed out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What happens if the length of the email message is too long? What is the solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with long documents, multiplying probabilities can lead to very small numbers, potentially causing underflow issues where the result is too small for the computer to represent. To avoid this, we use logarithms. Logarithms transform multiplications into additions, making calculations more manageable. The function \"predict_by_summation\" is the proper method that predicts a better label when dealing with long texts.\n",
    "You can see the difference when using this method in this [cell](#step4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. incorprating Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In codes provided, the Stop words are omitted from the BoW matrix thereby getting more accurate label predictions (this section is directly applied on the program in order to avoid any repetition). \n",
    "It is important to note that this program has used a previously made collection of Stop words in the nltk library (you will have to download Stop words first from the library).\n",
    "\n",
    "**However, without Stop words' removal, the result will be:**\n",
    "\n",
    "* Accuracy through multiplication: 0.7993019197207679\n",
    "* Accuracy through summation(logarithm): 0.8272251308900523\n",
    "\n",
    "which denotes the strong impact of Stop words on the classes. in this pragram the accuracy of the model falls by almost 15% if not implementing them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
